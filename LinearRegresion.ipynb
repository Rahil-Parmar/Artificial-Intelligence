{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegresion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zPELxCpHWtu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self):\n",
        "    self.w = np.random.uniform(-1,1,(2,))\n",
        "    self.error = []\n",
        "    self.l2_error = []\n",
        "\n",
        "  def mean_squared(self,X,target):\n",
        "    return np.sum((np.dot(X,self.w)-target)**2)/len(target)\n",
        "  \n",
        "  def gradient(self,X,y):\n",
        "    return (2/len(y))*np.sum(np.dot(X,self.w)-y)\n",
        "\n",
        "  def l2_regularization(self,X,y):\n",
        "    return (1/len(y))*np.sum((np.dot(X,self.w)-y)**2)\n",
        "\n",
        "  def l2_gradient(self,X,y):\n",
        "    return (1/len(y))* np.dot(X.T,(np.dot(X,self.w)-y))+(2*self.l*self.w)\n",
        "\n",
        "  def gradient_descent_l2(self,X,y,batch_size,alpha,l):\n",
        "    self.l = l\n",
        "    self.w = np.random.uniform(-1,1,(2,))\n",
        "    n = X.shape[0]//batch_size\n",
        "    i = 0\n",
        "    error = 0\n",
        "    for i in range(n+1):\n",
        "      input = X[i * batch_size:(i + 1)*batch_size, :]\n",
        "      target = y[i * batch_size:(i + 1)*batch_size]\n",
        "      self.w = self.w - alpha * self.l2_gradient(input,target)\n",
        "      error += self.l2_regularization(input,target)\n",
        "    if X.shape[0]%batch_size != 0:\n",
        "      input = X[i * batch_size:X.shape[0]]\n",
        "      target = y[i * batch_size:X.shape[0]]\n",
        "      self.w = self.w-alpha*self.l2_gradient(input,target)\n",
        "      error += self.l2_regularization(input,target)\n",
        "      i = i+1\n",
        "    return error/(i+1)\n",
        "\n",
        "  def fit_l2(self,X,y,batch_size=32,step = 100,alpha = 0.01,l=1):\n",
        "    data = np.hstack((np.ones((X.shape[0],1)),X[:,None]))\n",
        "    for i in range(step):\n",
        "      self.l2_error.append(self.gradient_descent_l2(data,y,batch_size,alpha,l))\n",
        "\n",
        "  def gradient_descent(self,X,y,batch_size,alpha):\n",
        "    n = X.shape[0]//batch_size\n",
        "    i = 0\n",
        "    error = 0\n",
        "    for i in range(n+1):\n",
        "      input = X[i * batch_size:(i + 1)*batch_size, :]\n",
        "      target = y[i * batch_size:(i + 1)*batch_size]\n",
        "      self.w = self.w - alpha * self.gradient(input,target)\n",
        "      error += self.mean_squared(input,target)\n",
        "    if X.shape[0]%batch_size != 0:\n",
        "      input = X[i * batch_size:X.shape[0]]\n",
        "      target = y[i * batch_size:X.shape[0]]\n",
        "      self.w = self.w-alpha*self.gradient(input,target)\n",
        "      error += self.mean_squared(input,target)\n",
        "      i = i+1\n",
        "    return error/(i+1)\n",
        "\n",
        "  def fit(self,X,y,batch_size=32,step = 100,alpha = 0.01):\n",
        "    data = np.hstack((np.ones((X.shape[0],1)),X[:,None]))\n",
        "    for i in range(step):\n",
        "      self.error.append(self.gradient_descent(data,y,batch_size,alpha))\n",
        "    \n",
        "  def predict(self,X):\n",
        "    return np.dot(np.hstack((np.ones((X.shape[0],1)),X[:,None])),self.w)"
      ],
      "metadata": {
        "id": "Xg2K6Ww1K0JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C5HuihTDJwza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
